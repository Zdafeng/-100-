## 深度学习100题（21-25题）21、 广义线性模型是怎被应用在深度学习中？>解析：  A Statistical View of Deep Learning (I): Recursive GLMs  深度学习从统计学角度，可以看做递归的广义线性模型。  广义线性模型相对于经典的线性模型(y=wx+b)，核心在于引入了连接函数g(.)，形式变为：y=g−1(wx+b)。  深度学习时递归的广义线性模型，神经元的激活函数，即为广义线性模型的链接函数。逻辑回归（广义线性模型的一种）的Logistic函数即为神经元激活函数中的Sigmoid函数，很多类似的方法在统计学和神经网络中的名称不一样，容易引起初学者（这里主要指我）的困惑。   下图是一个对照表：  ![image](https://github.com/Zdafeng/-100-/blob/master/image/21.1.jpg)  22、 如何解决梯度消失和梯度膨胀？  >解析：  
>（1）梯度消失：  根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都小于1的话，那么即使这个结果是0.99，在经过足够多层传播之后，误差对输入层的偏导会趋于0可以采用ReLU激活函数有效的解决梯度消失的情况，也可以用Batch Normalization解决这个问题。关于深度学习中 Batch Normalization为什么效果好？  （2）梯度膨胀  根据链式法则，如果每一层神经元对上一层的输出的偏导乘上权重结果都大于1的话，在经过足够多层传播之后，误差对输入层的偏导会趋于无穷大  可以通过激活函数来解决，或用Batch Normalization解决这个问题。23、简述神经网络的发展历史。  >解析：  1949年Hebb提出了神经心理学学习范式——Hebbian学习理论  1952年，IBM的Arthur Samuel写出了西洋棋程序  1957年，Rosenblatt的感知器算法是第二个有着神经系统科学背景的机器学习模型.  3年之后，Widrow因发明Delta学习规则而载入ML史册，该规则马上就很好的应用到了感知器的训练中感知器的热度在1969被Minskey一盆冷水泼灭了。他提出了著名的XOR问题，论证了感知器在类似XOR问题的线性不可分数据的无力。      尽管BP的思想在70年代就被Linnainmaa以“自动微分的翻转模式”被提出来，但直到1981年才被Werbos应用到多层感知器(MLP)中，NN新的大繁荣。  1991年的Hochreiter和2001年的Hochreiter的工作，都表明在使用BP算法时，NN单元饱和之后会发生梯度损失。又发生停滞。  时间终于走到了当下，随着计算资源的增长和数据量的增长。一个新的NN领域——深度学习出现了。  简言之，MP模型+sgn—->单层感知机（只能线性）+sgn— Minsky 低谷 —>多层感知机+BP+sigmoid—- (低谷) —>深度学习+pre-training+ReLU/sigmoid24、 深度学习常用方法。>解析：  全连接DNN（相邻层相互连接、层内无连接）：   AutoEncoder(尽可能还原输入)、Sparse Coding（在AE上加入L1规范）、RBM（解决概率问题）—–>特征探测器——>栈式叠加 贪心训练   RBM—->DBN   解决全连接DNN的全连接问题—–>CNN   解决全连接DNN的无法对时间序列上变化进行建模的问题—–>RNN—解决时间轴上的梯度消失问题——->LSTM  DNN是传统的全连接网络，可以用于广告点击率预估，推荐等。其使用embedding的方式将很多离散的特征编码到神经网络中，可以很大的提升结果。  CNN主要用于计算机视觉(Computer Vision)领域，CNN的出现主要解决了DNN在图像领域中参数过多的问题。同时，CNN特有的卷积、池化、batch normalization、Inception、ResNet、DeepNet等一系列的发展也使得在分类、物体检测、人脸识别、图像分割等众多领域有了长足的进步。同时，CNN不仅在图像上应用很多，在自然语言处理上也颇有进展，现在已经有基于CNN的语言模型能够达到比LSTM更好的效果。在最新的AlphaZero中，CNN中的ResNet也是两种基本算法之一。  GAN是一种应用在生成模型的训练方法，现在有很多在CV方面的应用，例如图像翻译，图像超清化、图像修复等等。  RNN主要用于自然语言处理(Natural Language Processing)领域，用于处理序列到序列的问题。普通RNN会遇到梯度爆炸和梯度消失的问题。所以现在在NLP领域，一般会使用LSTM模型。在最近的机器翻译领域，Attention作为一种新的手段，也被引入进来。  除了DNN、RNN和CNN外， 自动编码器(AutoEncoder)、稀疏编码(Sparse Coding)、深度信念网络(DBM)、限制玻尔兹曼机(RBM)也都有相应的研究。  25、 请简述神经网络的发展史。> 解析：  sigmoid会饱和，造成梯度消失。于是有了ReLU。  ReLU负半轴是死区，造成梯度变0。于是有了LeakyReLU，PReLU。  强调梯度和权值分布的稳定性，由此有了ELU，以及较新的SELU。  太深了，梯度传不下去，于是有了highway。  干脆连highway的参数都不要，直接变残差，于是有了ResNet。  强行稳定参数的均值和方差，于是有了BatchNorm。   在梯度流中增加噪声，于是有了 Dropout。  RNN梯度不稳定，于是加几个通路和门控，于是有了LSTM。  LSTM简化一下，有了GRU。  GAN的JS散度有问题，会导致梯度消失或无效，于是有了WGAN。  WGAN对梯度的clip有问题，于是有了WGAN-GP。  